#!/usr/bin/env python3
"""
Reporting Agent - The "Brain" of Revenue Leakage Detection System

This agent uses pure AI intelligence (BRAIN) to:
- Generate comprehensive, contextual reports from audit findings
- Create investigation tickets with intelligent prioritization
- Provide executive summaries with business insights
- Generate natural language explanations of findings
- Recommend business process improvements
- Create actionable intelligence from raw detections

The agent takes detection results from the Audit Analyst Agent and transforms
them into business-ready reports and actionable tickets.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional
import sqlite3
from dataclasses import dataclass
import json
import uuid
from jinja2 import Template
import warnings

warnings.filterwarnings('ignore')

@dataclass
class InvestigationTicket:
    """Investigation ticket generated by the Brain"""
    ticket_id: str
    title: str
    priority: str  # LOW, MEDIUM, HIGH, CRITICAL
    assigned_team: str
    customer_id: str
    contract_id: str
    leakage_type: str
    estimated_loss: float
    description: str
    investigation_steps: List[str]
    business_impact: str
    urgency_reason: str
    expected_resolution_time: str
    created_timestamp: datetime
    status: str  # OPEN, IN_PROGRESS, RESOLVED, CLOSED

@dataclass
class ExecutiveReport:
    """High-level executive report generated by the Brain"""
    report_id: str
    title: str
    executive_summary: str
    key_findings: List[str]
    financial_impact: Dict
    risk_assessment: str
    recommendations: List[str]
    next_steps: List[str]
    generated_timestamp: datetime

class ReportingAgent:
    """
    Reporting Agent - Pure AI Intelligence (BRAIN) for intelligent reporting
    
    This agent transforms technical audit findings into business intelligence:
    1. Natural language report generation
    2. Intelligent ticket creation and prioritization
    3. Executive summary generation
    4. Business impact analysis
    5. Actionable recommendations
    """
    
    def __init__(self, data_dir: str = None):
        self.logger = logging.getLogger(__name__)
        self.data_dir = Path(data_dir) if data_dir else Path(__file__).parent.parent / 'data'
        self.processed_dir = self.data_dir / 'processed'
        self.reports_dir = self.data_dir / 'reports'
        
        # Ensure directories exist
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Data containers
        self.detections_data = None
        self.tickets = []
        self.reports = []
        
        # Business intelligence configuration
        self.team_assignments = self._initialize_team_assignments()
        self.priority_rules = self._initialize_priority_rules()
        self.report_templates = self._initialize_report_templates()
        
        self.logger.info("Reporting Agent initialized - AI Brain ready for intelligent reporting!")
    
    def _initialize_team_assignments(self) -> Dict:
        """BRAIN: Intelligent team assignment rules"""
        return {
            'MISSING_CHARGES': 'Billing Operations Team',
            'INCORRECT_RATES': 'Revenue Assurance Team',
            'USAGE_MISMATCHES': 'Usage Analytics Team',
            'DUPLICATE_ENTRIES': 'Billing Operations Team',
            'STATISTICAL_ANOMALY': 'Data Analytics Team'
        }
    
    def _initialize_priority_rules(self) -> Dict:
        """BRAIN: Intelligent priority assignment rules"""
        return {
            'financial_thresholds': {
                'CRITICAL': 5000,
                'HIGH': 1000,
                'MEDIUM': 200,
                'LOW': 0
            },
            'urgency_factors': {
                'active_customer': 1.2,
                'high_tier_customer': 1.5,
                'recurring_issue': 1.3,
                'system_error': 1.4
            }
        }
    
    def _initialize_report_templates(self) -> Dict:
        """BRAIN: Intelligent report templates"""
        return {
            'executive_summary': Template("""
# Revenue Leakage Detection - Executive Summary

## Key Findings
{{ summary.total_detections }} revenue leakage issues identified with total estimated impact of ${{ "%.2f"|format(summary.total_estimated_loss) }}.

## Critical Issues Requiring Immediate Attention
- {{ summary.high_priority_count }} high-priority cases requiring urgent review
- Estimated immediate revenue at risk: ${{ "%.2f"|format(summary.critical_loss) }}

## Business Impact Assessment
{{ impact_assessment }}

## Recommended Actions
{{ recommendations }}
            """),
            
            'detailed_report': Template("""
# Revenue Leakage Detection - Detailed Analysis Report

Generated: {{ timestamp }}
Analysis Period: {{ period }}

## Executive Summary
{{ executive_summary }}

## Findings by Category
{% for category, details in findings_by_type.items() %}
### {{ category.replace('_', ' ').title() }}
- **Cases Found**: {{ details.count }}
- **Financial Impact**: ${{ "%.2f"|format(details.loss) }}
- **Average Confidence**: {{ "%.1f"|format(details.avg_confidence * 100) }}%

{% endfor %}

## High Priority Cases
{% for ticket in high_priority_tickets %}
### {{ ticket.title }}
- **Priority**: {{ ticket.priority }}
- **Estimated Loss**: ${{ "%.2f"|format(ticket.estimated_loss) }}
- **Assigned to**: {{ ticket.assigned_team }}
- **Description**: {{ ticket.description }}

{% endfor %}

## Recommendations for Business Process Improvement
{{ process_improvements }}
            """)
        }
    
    def load_audit_detections(self) -> pd.DataFrame:
        """BRAIN: Load and understand audit detection results"""
        self.logger.info("ðŸ§  Brain processing - Loading audit detection results...")
        
        # Try loading from CSV first
        csv_file = self.processed_dir / 'audit_detections.csv'
        if csv_file.exists():
            self.detections_data = pd.read_csv(csv_file)
            self.logger.info(f"ðŸ“Š Loaded {len(self.detections_data)} detections from {csv_file}")
        else:
            # Try SQLite database
            db_file = self.processed_dir / 'revenue_data.db'
            if db_file.exists():
                with sqlite3.connect(db_file) as conn:
                    self.detections_data = pd.read_sql_query("SELECT * FROM audit_detections", conn)
                self.logger.info(f"ðŸ“Š Loaded {len(self.detections_data)} detections from database")
            else:
                raise FileNotFoundError("No audit detection results found from Audit Analyst Agent")
        
        return self.detections_data
    
    def generate_investigation_tickets(self) -> List[InvestigationTicket]:
        """BRAIN: Generate intelligent investigation tickets"""
        self.logger.info("ðŸ§  Brain generating intelligent investigation tickets...")
        
        tickets = []
        
        for _, detection in self.detections_data.iterrows():
            # BRAIN: Intelligent priority calculation
            base_priority = self._calculate_base_priority(detection['estimated_loss'])
            adjusted_priority = self._apply_priority_adjustments(detection, base_priority)
            
            # BRAIN: Generate contextual title
            title = self._generate_ticket_title(detection)
            
            # BRAIN: Create investigation steps
            investigation_steps = self._generate_investigation_steps(detection)
            
            # BRAIN: Assess business impact
            business_impact = self._assess_business_impact(detection)
            
            # BRAIN: Determine urgency reasoning
            urgency_reason = self._determine_urgency_reason(detection, adjusted_priority)
            
            # BRAIN: Estimate resolution time
            resolution_time = self._estimate_resolution_time(detection, adjusted_priority)
            
            # BRAIN: Intelligent description enhancement
            enhanced_description = self._enhance_description(detection)
            
            ticket = InvestigationTicket(
                ticket_id=f"REV-{uuid.uuid4().hex[:8].upper()}",
                title=title,
                priority=adjusted_priority,
                assigned_team=self.team_assignments.get(detection['leakage_type'], 'Revenue Operations'),
                customer_id=detection['customer_id'],
                contract_id=detection['contract_id'],
                leakage_type=detection['leakage_type'],
                estimated_loss=detection['estimated_loss'],
                description=enhanced_description,
                investigation_steps=investigation_steps,
                business_impact=business_impact,
                urgency_reason=urgency_reason,
                expected_resolution_time=resolution_time,
                created_timestamp=datetime.now(),
                status='OPEN'
            )
            
            tickets.append(ticket)
        
        self.tickets = tickets
        self.logger.info(f"ðŸŽ¯ Generated {len(tickets)} intelligent investigation tickets")
        
        # BRAIN: Sort by priority and estimated impact
        self.tickets.sort(key=lambda t: (
            ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL'].index(t.priority),
            t.estimated_loss
        ), reverse=True)
        
        return tickets
    
    def _calculate_base_priority(self, estimated_loss: float) -> str:
        """BRAIN: Calculate base priority using intelligent rules"""
        thresholds = self.priority_rules['financial_thresholds']
        
        if estimated_loss >= thresholds['CRITICAL']:
            return 'CRITICAL'
        elif estimated_loss >= thresholds['HIGH']:
            return 'HIGH'
        elif estimated_loss >= thresholds['MEDIUM']:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _apply_priority_adjustments(self, detection: pd.Series, base_priority: str) -> str:
        """BRAIN: Apply intelligent priority adjustments based on context"""
        priority_score = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL'].index(base_priority)
        adjustment_factor = 1.0
        
        # Parse contextual analysis for priority factors
        context = str(detection.get('contextual_analysis', ''))
        
        # Higher priority for active customers
        if 'Active' in context:
            adjustment_factor *= self.priority_rules['urgency_factors']['active_customer']
        
        # Higher priority for premium tier customers
        if any(tier in context for tier in ['Premium', 'Enterprise', 'VIP']):
            adjustment_factor *= self.priority_rules['urgency_factors']['high_tier_customer']
        
        # Higher priority for system-flagged errors
        if 'System flagged' in context:
            adjustment_factor *= self.priority_rules['urgency_factors']['system_error']
        
        # Adjust priority score
        adjusted_score = min(3, int(priority_score * adjustment_factor))
        priorities = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']
        
        return priorities[adjusted_score]
    
    def _generate_ticket_title(self, detection: pd.Series) -> str:
        """BRAIN: Generate contextual ticket titles"""
        leakage_type = detection['leakage_type'].replace('_', ' ').title()
        customer_id = detection['customer_id']
        estimated_loss = detection['estimated_loss']
        
        title_templates = {
            'MISSING_CHARGES': f"Missing Charges - Customer {customer_id} - ${estimated_loss:.2f} Revenue at Risk",
            'INCORRECT_RATES': f"Rate Discrepancy - Customer {customer_id} - ${estimated_loss:.2f} Impact",
            'USAGE_MISMATCHES': f"Usage Billing Mismatch - Customer {customer_id} - ${estimated_loss:.2f}",
            'DUPLICATE_ENTRIES': f"Duplicate Billing - Customer {customer_id} - ${estimated_loss:.2f} Overcharge",
            'STATISTICAL_ANOMALY': f"Billing Anomaly - Customer {customer_id} - ${estimated_loss:.2f} Potential Impact"
        }
        
        return title_templates.get(detection['leakage_type'], f"{leakage_type} - Customer {customer_id}")
    
    def _generate_investigation_steps(self, detection: pd.Series) -> List[str]:
        """BRAIN: Generate intelligent investigation steps"""
        leakage_type = detection['leakage_type']
        
        step_templates = {
            'MISSING_CHARGES': [
                "1. Verify customer service activation status",
                "2. Check billing system configuration for the customer",
                "3. Review contract terms and pricing",
                "4. Generate missing invoices if confirmed",
                "5. Contact customer to explain billing correction"
            ],
            'INCORRECT_RATES': [
                "1. Review customer contract and pricing tier",
                "2. Verify current rate configuration in billing system",
                "3. Check for promotional rate expiry",
                "4. Calculate correct charges and variance",
                "5. Apply rate correction and notify customer"
            ],
            'USAGE_MISMATCHES': [
                "1. Audit usage metering system data",
                "2. Compare usage logs with billing records",
                "3. Verify overage calculation logic",
                "4. Check for metering system issues",
                "5. Correct billing and update processes"
            ],
            'DUPLICATE_ENTRIES': [
                "1. Identify and verify duplicate billing entries",
                "2. Check payment processing status",
                "3. Remove duplicate from billing system",
                "4. Issue customer credit if payment processed",
                "5. Review process to prevent future duplicates"
            ],
            'STATISTICAL_ANOMALY': [
                "1. Deep-dive analysis of billing pattern anomaly",
                "2. Cross-reference with contract and usage data",
                "3. Identify root cause of statistical deviation",
                "4. Verify calculations and system logic",
                "5. Implement corrective measures"
            ]
        }
        
        return step_templates.get(leakage_type, [
            "1. Review detection details and evidence",
            "2. Investigate underlying cause",
            "3. Verify findings with source systems",
            "4. Implement corrective actions",
            "5. Monitor for recurrence"
        ])
    
    def _assess_business_impact(self, detection: pd.Series) -> str:
        """BRAIN: Assess business impact with contextual intelligence"""
        estimated_loss = detection['estimated_loss']
        leakage_type = detection['leakage_type']
        severity = detection['severity']
        
        # Base impact assessment
        if estimated_loss >= 5000:
            financial_impact = "Significant financial impact on quarterly revenue"
        elif estimated_loss >= 1000:
            financial_impact = "Material impact requiring immediate attention"
        elif estimated_loss >= 200:
            financial_impact = "Moderate impact affecting monthly targets"
        else:
            financial_impact = "Minor impact but part of larger trend"
        
        # Type-specific impacts
        type_impacts = {
            'MISSING_CHARGES': "Direct revenue loss impacting top-line growth",
            'INCORRECT_RATES': "Pricing integrity issue affecting customer trust",
            'USAGE_MISMATCHES': "Metering accuracy concern impacting customer satisfaction",
            'DUPLICATE_ENTRIES': "Customer experience issue requiring immediate correction",
            'STATISTICAL_ANOMALY': "System integrity concern requiring investigation"
        }
        
        specific_impact = type_impacts.get(leakage_type, "Revenue assurance concern")
        
        return f"{financial_impact}. {specific_impact}. Severity: {severity}."
    
    def _determine_urgency_reason(self, detection: pd.Series, priority: str) -> str:
        """BRAIN: Determine and articulate urgency reasoning"""
        reasons = []
        
        if priority == 'CRITICAL':
            reasons.append("Financial impact exceeds critical threshold")
        elif priority == 'HIGH':
            reasons.append("Significant revenue at risk")
        
        # Context-based reasoning
        context = str(detection.get('contextual_analysis', ''))
        if 'Active' in context:
            reasons.append("affects active customer relationship")
        if 'System flagged' in context:
            reasons.append("system-detected error requiring validation")
        
        confidence = detection.get('confidence', 0)
        if confidence >= 0.9:
            reasons.append(f"high detection confidence ({confidence*100:.1f}%)")
        
        return "; ".join(reasons) if reasons else "Standard processing priority"
    
    def _estimate_resolution_time(self, detection: pd.Series, priority: str) -> str:
        """BRAIN: Estimate resolution time based on complexity and priority"""
        time_estimates = {
            'CRITICAL': '24 hours',
            'HIGH': '2-3 business days',
            'MEDIUM': '1 week',
            'LOW': '2 weeks'
        }
        
        base_time = time_estimates.get(priority, '1 week')
        
        # Adjust based on leakage type complexity
        complex_types = ['STATISTICAL_ANOMALY', 'USAGE_MISMATCHES']
        if detection['leakage_type'] in complex_types:
            return f"{base_time} (may require extended analysis)"
        
        return base_time
    
    def _enhance_description(self, detection: pd.Series) -> str:
        """BRAIN: Enhance description with business context"""
        base_description = detection['description']
        context = detection.get('contextual_analysis', '')
        recommended_action = detection.get('recommended_action', '')
        
        enhanced = f"{base_description}\n\n"
        enhanced += f"**Context**: {context}\n\n"
        enhanced += f"**Mathematical Evidence**: Confidence {detection.get('confidence', 0)*100:.1f}%\n\n"
        enhanced += f"**Recommended Action**: {recommended_action}"
        
        return enhanced
    
    def generate_executive_report(self) -> ExecutiveReport:
        """BRAIN: Generate executive-level business intelligence report"""
        self.logger.info("ðŸ§  Brain generating executive intelligence report...")
        
        if self.detections_data is None or len(self.detections_data) == 0:
            raise ValueError("No detection data available for report generation")
        
        # BRAIN: Calculate high-level metrics
        total_detections = len(self.detections_data)
        total_loss = self.detections_data['estimated_loss'].sum()
        high_priority_count = len(self.detections_data[
            self.detections_data['severity'].isin(['HIGH', 'CRITICAL'])
        ])
        critical_loss = self.detections_data[
            self.detections_data['severity'] == 'CRITICAL'
        ]['estimated_loss'].sum()
        
        # BRAIN: Generate key findings with business intelligence
        key_findings = self._generate_key_findings()
        
        # BRAIN: Create financial impact summary
        financial_impact = self._analyze_financial_impact()
        
        # BRAIN: Generate risk assessment
        risk_assessment = self._generate_risk_assessment()
        
        # BRAIN: Create intelligent recommendations
        recommendations = self._generate_recommendations()
        
        # BRAIN: Define next steps
        next_steps = self._generate_next_steps()
        
        # BRAIN: Create executive summary narrative
        executive_summary = self._generate_executive_summary_narrative(
            total_detections, total_loss, high_priority_count, critical_loss
        )
        
        report = ExecutiveReport(
            report_id=f"EXEC-RPT-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            title=f"Revenue Leakage Analysis - Executive Report",
            executive_summary=executive_summary,
            key_findings=key_findings,
            financial_impact=financial_impact,
            risk_assessment=risk_assessment,
            recommendations=recommendations,
            next_steps=next_steps,
            generated_timestamp=datetime.now()
        )
        
        self.reports.append(report)
        self.logger.info("ðŸŽ¯ Executive report generated with business intelligence")
        
        return report
    
    def _generate_key_findings(self) -> List[str]:
        """BRAIN: Generate intelligent key findings"""
        findings = []
        
        # Top leakage types
        by_type = self.detections_data.groupby('leakage_type').agg({
            'estimated_loss': 'sum',
            'detection_id': 'count'
        }).sort_values('estimated_loss', ascending=False)
        
        top_type = by_type.index[0]
        top_loss = by_type.iloc[0]['estimated_loss']
        findings.append(f"{top_type.replace('_', ' ').title()} represents the highest risk category with ${top_loss:.2f} in potential losses")
        
        # Severity distribution
        critical_count = len(self.detections_data[self.detections_data['severity'] == 'CRITICAL'])
        if critical_count > 0:
            findings.append(f"{critical_count} critical issues requiring immediate executive attention")
        
        # Customer impact
        affected_customers = self.detections_data['customer_id'].nunique()
        findings.append(f"{affected_customers} customers affected by revenue leakage issues")
        
        # Confidence analysis
        high_confidence = len(self.detections_data[self.detections_data['confidence'] >= 0.8])
        findings.append(f"{high_confidence} cases identified with high confidence (>80%) requiring immediate action")
        
        return findings
    
    def _analyze_financial_impact(self) -> Dict:
        """BRAIN: Analyze financial impact with business context"""
        total_loss = self.detections_data['estimated_loss'].sum()
        
        # Monthly/quarterly projections
        monthly_impact = total_loss  # Assuming detections represent monthly data
        quarterly_impact = monthly_impact * 3
        annual_projection = monthly_impact * 12
        
        # Impact by severity
        by_severity = self.detections_data.groupby('severity')['estimated_loss'].sum()
        
        return {
            'immediate_loss': round(total_loss, 2),
            'monthly_impact': round(monthly_impact, 2),
            'quarterly_projection': round(quarterly_impact, 2),
            'annual_projection': round(annual_projection, 2),
            'by_severity': by_severity.to_dict(),
            'recovery_potential': round(total_loss * 0.85, 2)  # Assuming 85% recovery rate
        }
    
    def _generate_risk_assessment(self) -> str:
        """BRAIN: Generate intelligent risk assessment"""
        total_loss = self.detections_data['estimated_loss'].sum()
        critical_count = len(self.detections_data[self.detections_data['severity'] == 'CRITICAL'])
        
        if total_loss >= 50000:
            risk_level = "HIGH RISK"
            risk_desc = "Significant revenue leakage threatening quarterly targets"
        elif total_loss >= 20000:
            risk_level = "MEDIUM-HIGH RISK"
            risk_desc = "Material revenue impact requiring immediate management attention"
        elif total_loss >= 5000:
            risk_level = "MEDIUM RISK"
            risk_desc = "Notable revenue gaps requiring systematic resolution"
        else:
            risk_level = "LOW-MEDIUM RISK"
            risk_desc = "Manageable issues within normal operational variance"
        
        additional_risks = []
        if critical_count > 5:
            additional_risks.append("Multiple critical issues suggest systemic problems")
        
        unique_types = self.detections_data['leakage_type'].nunique()
        if unique_types >= 4:
            additional_risks.append("Multiple leakage types indicate broad system vulnerabilities")
        
        risk_factors = ". ".join(additional_risks) if additional_risks else "No additional systemic risks identified"
        
        return f"{risk_level}: {risk_desc}. {risk_factors}."
    
    def _generate_recommendations(self) -> List[str]:
        """BRAIN: Generate intelligent business recommendations"""
        recommendations = []
        
        # Priority-based recommendations
        critical_count = len(self.detections_data[self.detections_data['severity'] == 'CRITICAL'])
        if critical_count > 0:
            recommendations.append(f"Immediate action required: Address {critical_count} critical issues within 24 hours")
        
        # Type-specific recommendations
        by_type = self.detections_data.groupby('leakage_type').size().sort_values(ascending=False)
        top_type = by_type.index[0]
        
        type_recommendations = {
            'MISSING_CHARGES': "Implement automated billing completeness checks",
            'INCORRECT_RATES': "Review and update rate configuration processes",
            'USAGE_MISMATCHES': "Audit usage metering and billing integration",
            'DUPLICATE_ENTRIES': "Enhance billing system duplicate prevention",
            'STATISTICAL_ANOMALY': "Deploy advanced analytics monitoring"
        }
        
        recommendations.append(type_recommendations.get(top_type, "Address primary leakage category"))
        
        # Process improvements
        recommendations.append("Establish regular revenue assurance monitoring cadence")
        recommendations.append("Implement preventive controls to reduce future leakage")
        
        return recommendations
    
    def _generate_next_steps(self) -> List[str]:
        """BRAIN: Generate intelligent next steps"""
        return [
            "Review and approve investigation ticket assignments",
            "Monitor critical case resolution within 24-hour SLA",
            "Schedule weekly revenue assurance review meetings",
            "Establish KPIs for revenue leakage prevention",
            "Plan system improvements based on root cause analysis"
        ]
    
    def _generate_executive_summary_narrative(self, total: int, loss: float, high_priority: int, critical_loss: float) -> str:
        """BRAIN: Generate narrative executive summary"""
        return f"""
        Our AI-powered revenue leakage detection system has identified {total} instances of potential revenue 
        leakage with an estimated total impact of ${loss:.2f}. Of these, {high_priority} cases are classified 
        as high priority, representing ${critical_loss:.2f} in immediate revenue at risk.
        
        This analysis indicates {'significant' if loss > 20000 else 'moderate' if loss > 5000 else 'minor'} 
        revenue assurance challenges requiring {'immediate' if high_priority > 10 else 'prompt'} management attention. 
        The automated detection and prioritization system has enabled proactive identification of issues that 
        would otherwise impact our revenue integrity.
        
        Immediate action on the highest priority cases could result in revenue recovery of up to ${loss * 0.85:.2f}, 
        representing a significant return on our revenue assurance investment.
        """
    
    def generate_detailed_report(self) -> str:
        """BRAIN: Generate comprehensive detailed report"""
        self.logger.info("ðŸ§  Brain generating comprehensive detailed report...")
        
        # Prepare data for template
        summary_data = {
            'total_detections': len(self.detections_data),
            'total_estimated_loss': self.detections_data['estimated_loss'].sum(),
            'high_priority_count': len(self.detections_data[
                self.detections_data['severity'].isin(['HIGH', 'CRITICAL'])
            ]),
            'critical_loss': self.detections_data[
                self.detections_data['severity'] == 'CRITICAL'
            ]['estimated_loss'].sum()
        }
        
        # Findings by type
        findings_by_type = {}
        for leakage_type in self.detections_data['leakage_type'].unique():
            type_data = self.detections_data[self.detections_data['leakage_type'] == leakage_type]
            findings_by_type[leakage_type] = {
                'count': len(type_data),
                'loss': type_data['estimated_loss'].sum(),
                'avg_confidence': type_data['confidence'].mean()
            }
        
        # High priority tickets
        high_priority_tickets = [t for t in self.tickets if t.priority in ['HIGH', 'CRITICAL']][:10]
        
        # Generate content sections
        executive_summary = self._generate_executive_summary_narrative(
            summary_data['total_detections'],
            summary_data['total_estimated_loss'],
            summary_data['high_priority_count'],
            summary_data['critical_loss']
        )
        
        impact_assessment = self._generate_risk_assessment()
        process_improvements = self._generate_process_improvement_recommendations()
        
        # Render report using template
        report_content = self.report_templates['detailed_report'].render(
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            period="Current Analysis Period",
            executive_summary=executive_summary,
            findings_by_type=findings_by_type,
            high_priority_tickets=high_priority_tickets,
            process_improvements=process_improvements
        )
        
        return report_content
    
    def _generate_process_improvement_recommendations(self) -> str:
        """BRAIN: Generate process improvement recommendations"""
        recommendations = []
        
        # Analyze patterns for improvements
        by_type = self.detections_data['leakage_type'].value_counts()
        
        for leakage_type, count in by_type.items():
            if count >= 5:  # Frequent occurrence suggests process issue
                improvement_map = {
                    'MISSING_CHARGES': "Implement automated billing validation and completeness checks",
                    'INCORRECT_RATES': "Establish rate change approval workflows and validation rules", 
                    'USAGE_MISMATCHES': "Deploy real-time usage-to-billing reconciliation monitoring",
                    'DUPLICATE_ENTRIES': "Enhance billing system with duplicate detection and prevention",
                    'STATISTICAL_ANOMALY': "Create ML-powered anomaly detection for continuous monitoring"
                }
                
                if leakage_type in improvement_map:
                    recommendations.append(f"**{leakage_type.replace('_', ' ').title()}** ({count} cases): {improvement_map[leakage_type]}")
        
        # Add general recommendations
        recommendations.extend([
            "**Preventive Controls**: Implement upstream validation to catch issues before billing",
            "**Monitoring Dashboard**: Create real-time revenue assurance monitoring dashboard",
            "**Training Program**: Establish revenue assurance training for billing operations team"
        ])
        
        return "\n".join([f"{i+1}. {rec}" for i, rec in enumerate(recommendations)])
    
    def save_reports_and_tickets(self) -> bool:
        """Save all generated reports and tickets"""
        try:
            # Save tickets to CSV
            if self.tickets:
                ticket_records = []
                for ticket in self.tickets:
                    record = {
                        'ticket_id': ticket.ticket_id,
                        'title': ticket.title,
                        'priority': ticket.priority,
                        'assigned_team': ticket.assigned_team,
                        'customer_id': ticket.customer_id,
                        'contract_id': ticket.contract_id,
                        'leakage_type': ticket.leakage_type,
                        'estimated_loss': ticket.estimated_loss,
                        'description': ticket.description,
                        'investigation_steps': json.dumps(ticket.investigation_steps),
                        'business_impact': ticket.business_impact,
                        'urgency_reason': ticket.urgency_reason,
                        'expected_resolution_time': ticket.expected_resolution_time,
                        'status': ticket.status,
                        'created_timestamp': ticket.created_timestamp.isoformat()
                    }
                    ticket_records.append(record)
                
                tickets_df = pd.DataFrame(ticket_records)
                tickets_file = self.reports_dir / 'investigation_tickets.csv'
                tickets_df.to_csv(tickets_file, index=False)
                self.logger.info(f"ðŸ’¾ Saved {len(tickets_df)} tickets to {tickets_file}")
                
                # Save to database
                db_file = self.processed_dir / 'revenue_data.db'
                with sqlite3.connect(db_file) as conn:
                    tickets_df.to_sql('investigation_tickets', conn, if_exists='replace', index=False)
            
            # Save detailed report
            detailed_report = self.generate_detailed_report()
            detailed_report_file = self.reports_dir / f'detailed_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
            with open(detailed_report_file, 'w', encoding='utf-8') as f:
                f.write(detailed_report)
            self.logger.info(f"ðŸ’¾ Saved detailed report to {detailed_report_file}")
            
            # Save executive reports
            for report in self.reports:
                report_data = {
                    'report_id': report.report_id,
                    'title': report.title,
                    'executive_summary': report.executive_summary,
                    'key_findings': json.dumps(report.key_findings),
                    'financial_impact': json.dumps(report.financial_impact),
                    'risk_assessment': report.risk_assessment,
                    'recommendations': json.dumps(report.recommendations),
                    'next_steps': json.dumps(report.next_steps),
                    'generated_timestamp': report.generated_timestamp.isoformat()
                }
                
                exec_report_file = self.reports_dir / f'executive_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                with open(exec_report_file, 'w', encoding='utf-8') as f:
                    json.dump(report_data, f, indent=2)
                self.logger.info(f"ðŸ’¾ Saved executive report to {exec_report_file}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to save reports and tickets: {str(e)}")
            return False
    
    def get_reporting_summary(self) -> Dict:
        """Get summary of reporting results"""
        return {
            'agent_type': 'Reporting Agent (Brain)',
            'processing_timestamp': datetime.now().isoformat(),
            'total_detections_processed': len(self.detections_data) if self.detections_data is not None else 0,
            'investigation_tickets_created': len(self.tickets),
            'executive_reports_generated': len(self.reports),
            'high_priority_tickets': len([t for t in self.tickets if t.priority in ['HIGH', 'CRITICAL']]),
            'critical_tickets': len([t for t in self.tickets if t.priority == 'CRITICAL']),
            'total_estimated_loss': sum(t.estimated_loss for t in self.tickets),
            'reports_saved': True if self.tickets and self.reports else False
        }
    
    def run_full_reporting(self) -> Tuple[List[InvestigationTicket], ExecutiveReport, Dict]:
        """Run complete reporting pipeline"""
        self.logger.info("ðŸš€ Starting Reporting Agent - Full Intelligence Pipeline")
        
        try:
            # Step 1: Load audit detections
            self.load_audit_detections()
            
            # Step 2: Generate investigation tickets
            tickets = self.generate_investigation_tickets()
            
            # Step 3: Generate executive report
            exec_report = self.generate_executive_report()
            
            # Step 4: Save all outputs
            self.save_reports_and_tickets()
            
            # Step 5: Generate summary
            summary = self.get_reporting_summary()
            
            self.logger.info("ðŸ§  Reporting Agent intelligence processing complete!")
            self.logger.info(f"ðŸ“Š Generated {len(tickets)} tickets and {len(self.reports)} reports")
            
            return tickets, exec_report, summary
            
        except Exception as e:
            self.logger.error(f"Reporting pipeline failed: {str(e)}")
            raise

# Agent initialization function
def create_reporting_agent(data_dir: str = None) -> ReportingAgent:
    """Factory function to create Reporting Agent"""
    return ReportingAgent(data_dir=data_dir)

if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Create and run agent
    agent = create_reporting_agent()
    tickets, exec_report, summary = agent.run_full_reporting()
    
    print("\n" + "="*60)
    print("REPORTING AGENT - INTELLIGENCE SUMMARY")
    print("="*60)
    for key, value in summary.items():
        print(f"{key.replace('_', ' ').title()}: {value}")
    print("="*60)
    
    if tickets:
        print("\nTOP PRIORITY TICKETS:")
        for ticket in tickets[:5]:
            print(f"  {ticket.priority}: {ticket.title} - ${ticket.estimated_loss:.2f}")
    
    if exec_report:
        print(f"\nEXECUTIVE REPORT: {exec_report.title}")
        print(f"Key Findings: {len(exec_report.key_findings)} items")
        print(f"Financial Impact: ${exec_report.financial_impact.get('immediate_loss', 0):.2f}")
